% !TeX spellcheck = en_US
\section{Problem 14}

We are given an abstract of a CNN that classifies images into two classes. Its structure is as follows:
\begin{itemize}
	\item \textbf{Input}: $100 \times 100$ grayscale images.
	\item \textbf{Layer 1}: Convolutional layer with $100 \ 5\times 5$ convolutional filters.
	\item \textbf{Layer 2}: Convolutional layer with $100 \ 5\times 5$ convolutional filters.
	\item \textbf{Layer 3}: Max Pooling layer with reduction of $2$.
	\item \textbf{Layer 4}: Dense layer with 100 units.
	\item \textbf{Layer 5}: Dense layer with 100 units.
	\item \textbf{Layer 6}: Single output unit.
\end{itemize}

In order to calculate all the weights in this CNN, we have to consider each layer separately:\\

\begin{minipage}[l]{0.47\textwidth}
	\textbf{Layer 1:}
	\begin{itemize}
		\item Input size: $100 \times 100$.
		\item Filter size: $5 \times 5$.
		\item Number of filters: $100$.
		\item Weights: Each filter has $5 \times 5$ weights and there's a bias per filter.
		\begin{itemize}
			\item Weights per filter: $5 \times 5=25$.
			\item Total weights: $25\times 100=2500$.
			\item Total biases: $100$ (1 per filter).
		\end{itemize}
	\end{itemize}
	So, this layer produces shape $\left(96,96,100\right)$ and in total we have $2500 + 100 = 2600$ weights. \\ {\small Output shape is calculated as: input num $ - $ kernel size $+ 1$}
\end{minipage}
\hfil
\begin{minipage}[r]{0.47\textwidth}
	\textbf{Layer 2:}
	\begin{itemize}
		\item Input channels: $100$ (from layer 1).
		\item Filter size: $5 \times 5$.
		\item Number of filters: $100$.
		\item Weights: Each filter has $5 \times 5$ weights for each input channel.
		\begin{itemize}
			\item Weights per filter: $5 \times 5 \times 100 = 2500$.
			\item Total weights: $2500\times 100 = \num{250000}$.
			\item Total biases: $100$ (1 per filter).
		\end{itemize}
	\end{itemize}
	In total, we have $\num{250000} + 100 = \num{250100}$ weights and it creates an output shape of $\left(92,92,100\right)$.
\end{minipage}

\vspace{3mm}

Moving on to \textbf{Layer 3}, it's important to note that this layer doesn't have any weights or biases because it's a pooling layer. Pooling layers downsample the output from the previous layer.
In this case, Layer's 2 output is reduces to $46 \times 46 \times 100$  from $92 \times 92 \times 100$.\\

\textbf{Layer 4} is a dense (\textit{fully connected}) layer with an input that of the max pool layer. Before it connects with the max pool layer, the data must be converted from multi-dimensional array into a one-dimensional array.
After this is done, the input data of layer 4 have a size of $46 \times 46 \times 100 = \num{211600}$. So, in order to calculate the weights and biases, we only need two information: the number of neurons ($100$) and the number of neurons of the previous layer ($\num{211600}$).\\
The equation for total weights of this layer is: \\
$\textit{number of neurons} \times \textit{number of neurons of the previous layer} + \textit{number of neurons} = 100 \times 211600 + 100 = 21160100$. \\ 

\textbf{Layer 5} is also a dense layer and the procedure for calculating the weights is the same as above. 
We have $100$ units in this layer and $100$ in the last one, so total weights are: $100 \times 100 + 100 = \num{10100}$. \\

Moving on to the \textbf{output layer}, weights are equal to the input units and bias is only $1$, so this layer's weights number is $100 +1 = 101$. \\ 

The total number of weights is:
\[
\begin{gathered}
\text{Total Weights} = \text{Layer 1} + \text{Layer 2} + \text{Layer 3} + \text{Layer 4} + \text{Layer 5} + \text{Layer 6} = \\
= \num{2600} + \num{250100} + \num{21160100} + \num{10100} + \num{101} = \\  =\mathbf{\num{21423001}} \text{ weights}
\end{gathered}
\]
\vspace{3mm}