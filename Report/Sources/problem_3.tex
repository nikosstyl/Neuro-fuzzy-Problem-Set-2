% !TeX spellcheck = en_US
\section{Problem 3}

For the given neural network, we have:
\begin{itemize}
	\item $w^1\left(0\right) = -3,\ w^2\left(0\right) = -1$,
	\item $b^1\left(0\right) = 2,\ b^2\left(0\right) = -1$ and
	\item input/target pair $\left\{p=1,\ t=0\right\}$
\end{itemize}
\vspace*{1mm}
The output of the first layer is:
\[
\begin{gathered}
n^1 = w^1 p + b^1 = (-3)(1) + 2 = -1\\
a^1 = \text{Swish}\left(n^1\right) = \text{Swish}\left(-1\right) = \dfrac{n^1}{1+e^{-n^1}} = \dfrac{-1}{1+e} = -0.2689
\end{gathered}
\]

The output of the second layer is then:
\[
\begin{gathered}
n^2 = w^2 a^1 + b^2 = (-1)(-0.2689) + (-1) = -0.7311 \\ 
a^2 = \text{LReLU}\left(n^2\right) = \text{LReLU}\left(-0.7311\right) = 0.001
\end{gathered}
\]
So, the error calculated is:
\[
e = t-a^2 = \left(1-\left(0.001\right)\right) = 0.999 \approx 1
\]

Now, we can apply back-propagation starting from the second layer:
\[
s^2 = -2 \text{LReLU}^{'}\left(n^2\right)\left(t-a\right)
\]