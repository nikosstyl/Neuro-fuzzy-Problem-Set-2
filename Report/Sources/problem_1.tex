% !TeX spellcheck = en_US
\section{Problem 1}
In this exercise we need to find the minimum of the given 2-dimensional function:\\
\begin{equation}	
		F(\mathbf{w})=w_{1}^{2}+w_{2}^{2}+(0.5w_{1}+w_{2})^{2}+(0.5w_{1}+w_{2})^{4}	
		\label{eq:function1}
\end{equation}

with the Conjugate Gradient (Fletcher-Reeves) method and the Gradient Descent.\\

Initially, we can conclude that the function $F(w)$ is not in quadratic form because of the term $(0.5w_{1}+w_{2})^{4}$.
A function is said to be in quadratic form if it can be expressed as a second-degree polynomial where all the terms are either squared terms or cross-products of the variables. The presence of the fourth-degree term $(0.5w_{1}+w_{2})^{4}$.
makes this function a higher-degree polynomial, specifically a quartic function with respect to $(0.5w_{1}+w_{2})$, which means it cannot be classified as quadratic.\\
Also, the independent values in this function are $w_{1},w_{2}$, because only with them we can manipulate the $F(w)$.\\ 

As an initial values we have $w\left(0\right) = \left[3, 3\right]^T$ and  $tol=\num{1e-6}$.\\

\subsection{Conjugate Gradient}
Here, we have to use Conjugate Gradient (\textit{GD}) method in order to find the minimum of it.
The steps we have to use are specific for each iteration.

\begin{center}
	\underline{\textit{ITERATION} $k = 0$}
\end{center}

\underline{Step1: Calculate the Gradient at  $w\left(x_k\right)$ }\\
\[
\begin{gathered}
	\nabla f(w_1,w_2) = \left(\begin{array}{c}
		\dfrac{\partial f}{\partial w_1} \\[4mm]
		\dfrac{\partial f}{\partial w_2}
	\end{array}\right) = \left(\begin{array}{c}
		2w_1 + (0.5w_1+w_2) + 2(0.5w_1+w_2)^3\\[1mm]
		2w_2 + 2(0.5w_1+w_2) + 4(0.5w_1+w_2)^3
	\end{array}\right) =\\= \left(\begin{array}{c}
		2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
		w_1 + 4w_2 + 4(0.5w_1+w_2)^3
	\end{array}\right)
\end{gathered}
\]\\[3mm]

where at the point $w\left(0\right) = \left[3, 3\right]^T$ we have $\nabla f(w) = \left(\begin{array}{c}
	192.75 \\
	379.5
\end{array}\right)$\\
Thus, the search direction is $s_k = -\nabla f\left( w^{(k)}\right) = [\num{-192.75} \ \num{-379.5}]^T$.\\

\underline{Step 2: Find $\lambda_k$}\\
Using a line search algorithm (\textit{ie backtracking}), we find the optimal $\lambda_k$ for this iteration. This algorithm goes as follows:
\begin{algorithm}[H]
	\caption{Backtracking Line Search}
	\begin{algorithmic}
		\State Choose $\lambda > 0$, $0 < \rho < 1$, and $0 < c < 1$.
		\While{$f(w + \lambda p) > f(w) + c \cdot \lambda \cdot \nabla f(x)^T \cdot p$}
		\State $\lambda \gets \rho \cdot \lambda$
		\EndWhile
	\end{algorithmic}
\end{algorithm}
In order to use this algorithm we set $\lambda_{init} = 1, \rho=0.5, c=\num{1e-4}$.
Thus, for $k=0$, $\lambda_0 = 0.01562$.\\

\underline{Step 3: Calculate next step $w_{1}$}\\
Now that we have $\lambda_k$, finding next $w_k$ is easy. It is calculated as
\[
w_{k+1} = w_k + \lambda_k s_k = [3\ 3]^T + 0.01562 \cdot [\num{-192.75} \ \num{-379.5}]^T = [-0.0117\  -2.9297]^T
\]
Iteration number $k$ is updated to $k+1=1$ and search direction is updated using the equation below:
\[
\begin{gathered}
	s_{k} = - \nabla f(w_k) + \omega_k \cdot s_{k-1} = [36.936 \ 80.203]^T, \quad \text{where} \\ 
	\omega_k = \dfrac{\nabla^T f(w_k) \cdot \nabla f(w_k)}{\nabla^T f(w_{k-1}) \cdot \nabla f(w_{k-1})} = \dfrac{\left( \nabla f(w_k) \right)^2}{\left( \nabla f(w_{k-1}) \right)^2} =  0.086207
\end{gathered}
\]

\underline{Step 4: Check for convergence}\\[2mm]
$||s_k|| =  88.299 > \epsilon \approx \num{1e-6}$, thus convergence is not achieved.\\[3mm]

\begin{center}
	\underline{\textit{ITERATION} $k=1$}
\end{center}

Search direction has been already calculated in previous iteration, so steps change a bit from now on.

\underline{Step 1: Find $\lambda_k$}
\[
\lambda_1 = \textit{backtracking} = 0.031250
\]

\underline{Step 2: Calculate next step $w_{2}$}
\[
w_{k+1} = w_k + \lambda_k s_k = [-0.0117\  -2.9297]^T + 0.3125 \cdot [36.936\  80.203]^T = [1.142542\ -0.423361]^T
\]
Iteration number is increased to $2$ and search direction is updated to:
\[
s_k = [-2.4247\ 0.57]^T
\]

\underline{Step 3: Check for convergence}\\[2mm]
$||s_k|| = 2.4908 > \epsilon$, thus no convergence.\\[3mm]


\begin{center}
	\underline{\textit{ITERATION} $k=2$}
\end{center}

\underline{Step 1: Find $\lambda_k$}
\[
\lambda_2 = \textit{backtracking} = 0.5
\]

\underline{Step 2: Calculate next step $w_{3}$}
\[
w_{k+1} = w_k + \lambda_k s_k = [1.142542\ -0.423361]^T + 0.5 \cdot [âˆ’2.4247\  0.57]^T = [-0.069812\ -0.138359]^T
\]
Iteration number is increased to $3$ and search direction is updated to:
\[
s_k = [0.1215\ 0.69148]^T
\]

\underline{Step 3: Check for convergence}\\[2mm]
$||s_k|| = 0.702082 > \epsilon$, thus no convergence.\\[3mm]


\begin{center}
	\underline{\textit{ITERATION $k=3$}}
\end{center}

\underline{Step 1: Find $\lambda_k$}
\[
\lambda_3 = \textit{backtracking} = 0.25
\]

\underline{Step 2: Calculate next step $w_{4}$}
\[
w_{k+1} = w_k + \lambda_k s_k = [-0.069812\ -0.138359]^T + 0.25 \cdot [0.1215\  0.69148]^T = [-0.039434\ 0.034513]^T
\]
Iteration number is increased to $4$ and search direction is updated to:
\[
s_k = [0.0673\ -0.08021]^T
\]

\underline{Step 3: Check for convergence}\\[2mm]
$||s_k|| = 0.1047 > \epsilon$, thus no convergence.\\[3mm]


\begin{center}
	\underline{\textit{ITERATION $k=4$}}
\end{center}

\underline{Step 1: Find $\lambda_k$}
\[
\lambda_4 = \textit{backtracking} = 0.5
\]

\underline{Step 2: Calculate next step $w_{5}$}
\[
w_{k+1} = w_k + \lambda_k s_k = [-0.039434\ 0.034513]^T + 0.5 \cdot [0.0673\ -0.08021]^T = [-0.005783\ -0.005593]^T
\]
Iteration number is increased to $5$ and search direction is updated to:
\[
s_k = [0.025861\ 0.021228]^T
\]

\underline{Step 3: Check for convergence}\\[2mm]
$||s_k|| = 0.03346 > \epsilon$, thus no convergence.\\[3mm]

Normally, this procedure will continue until the \nth{23} iteration with the calculated minimum point being:
\[
w_{final} = \left( \num{5.544e-7}, \num{-3.9125e-7} \right) \approx (0,0)
\]

\subsection{Gradient Descent}
%\begin{center}
%	\underline{\textbf{GRADIENT DESCENT}}
%\end{center}
Gradient descent is one of the most favored optimization technique, because of its simplicity and its generality.\\
In order to find the minimum of the Function ~\ref{eq:function1} we must first initialize our initial point $w_{0}$, value of tolerance $tol$ and Step size\\
As we have mentioned, the initial values are:
\begin{itemize}
	\item $w\left(0\right) = \left[3, 3\right]^T$
	\item $tol = 10^{-6}$
	\item $\alpha = 0.01$\\
	The exercise says that we apply the GD method with unit step movement. However, if we set $\alpha = 1$, which is considered a large value, the algorithm may not converge to the optimal point and it will even diverge completely. That's why we set $\alpha = 0.01$. 
\end{itemize} 
\vspace{2mm}

Taking these initializations into account, we follow the Gradient Descent's steps:\\

\underline{Step 1: Compute gradient $\nabla f$ at $w_{k}$}
\vspace{4mm}

\underline{Step 2: Make a scaled step in the opposite direction to the gradient}\\
\begin{equation}
	\begin{gathered}
		\text{step} = \alpha \cdot \nabla f_{k}\\
	\end{gathered}
\end{equation}
\vspace{2mm}

\underline{Step 3: Check convergence}\\
If the step size is smaller than the convergence, then we stop. Else, we repeat Step 1 and Step 2 while we update $x$.
\vspace{4mm}

\underline{Step 4: Update $x$}
\begin{equation}
	x_{k+1} = x_{k} - \alpha \cdot \nabla f_{k}
\end{equation}
\vspace{4mm}

Taking these steps into consideration we will attempt to minimize our function.\\

\begin{center}
	\underline{\textit{ITERATION k = 0}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) = $\left(\begin{array}{c}
	2w_1 + (0.5w_1+w_2) + 2(0.5w_1+w_2)^3\\[1mm]
	2w_2 + 2(0.5w_1+w_2) + 4(0.5w_1+w_2)^3
\end{array}\right) = \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(0\right) = \left[3, 3\right]^T$ we have $\nabla f_{0} = \left(\begin{array}{c}
	192.75 \\
	379.5
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{0} = 0.01 \cdot \left(\begin{array}{c}
	192.75 \\
	379.5
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
1.92750 \\
3.7950
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = 4.256440 > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_1 = w_0 - a\cdot \nabla f_0 =  \left[\begin{array}{c}
	3 \\
	3
\end{array}\right] - \left(\begin{array}{c}
1.92750 \\
3.7950
\end{array}\right) \rightarrow
w_1 = \left[\begin{array}{c}
	 1.073\\
	 -0.795
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 1}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(1\right) = \left[1.073, -0.795\right]^T$ we have $\nabla f_{1} = \left(\begin{array}{c}
	1.851603 \\
	-2.176795
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{1} = 0.01 \cdot \left(\begin{array}{c}
1.851603 \\
-2.176795
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
	0.018516 \\
	-0.021768
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.857773e-02 }> tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_2 = w_1 - a\cdot \nabla f_1 =  \left[\begin{array}{c}
 1.073\\
-0.795
\end{array}\right] - \left(\begin{array}{c}
	0.018516 \\
	-0.021768
\end{array}\right) \rightarrow
w_2 = \left[\begin{array}{c}
	1.054\\
	-0.773
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 2}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(2\right) = \left[1.054, -0.773\right]^T$ we have $\nabla f_{2} = \left(\begin{array}{c}
	1.831867 \\
	-2.098666
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{2} = 0.01 \cdot \left(\begin{array}{c}
	1.831867 \\
   -2.098666
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
	0.018319 \\
	-0.020987
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.785702e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_3 = w_2 - a\cdot \nabla f_2 =  \left[\begin{array}{c}
	1.054 \\
	-0.773
\end{array}\right] - \left(\begin{array}{c}
	0.018319 \\
-0.020987
\end{array}\right) \rightarrow
w_3 = \left[\begin{array}{c}
	1.036\\
	-0.752
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 3}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(3\right) = \left[1.036, -0.752\right]^T$ we have $\nabla f_{3} = \left(\begin{array}{c}
	1.811156 \\
	-2.024840
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{3} = 0.01 \cdot \left(\begin{array}{c}
	1.811156 \\
-2.024840
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
	0.018112 \\
	-0.020248
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.716664e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_4 = w_3 - a\cdot \nabla f_3 =  \left[\begin{array}{c}
	1.036\\
	-0.752
\end{array}\right] - \left(\begin{array}{c}
	0.018319 \\
	-0.020987
\end{array}\right) \rightarrow
w_4 = \left[\begin{array}{c}
	1.018\\
	-0.732
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 4}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(4\right) = \left[1.018, -0.732\right]^T$ we have $\nabla f_{4} = \left(\begin{array}{c}
	1.789642 \\
	-1.954924
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{4} = 0.01 \cdot \left(\begin{array}{c}
	1.789642 \\
-1.954924
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
	0.017896 \\
	-0.019549
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.650386e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_5 = w_4 - a\cdot \nabla f_4 =  \left[\begin{array}{c}
	1.018\\
	-0.732
\end{array}\right] - \left(\begin{array}{c}
	0.017896 \\
-0.019549
\end{array}\right) \rightarrow
w_5 = \left[\begin{array}{c}
	1.000\\
	-0.712
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 5}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(5\right) = \left[1.000, -0.712\right]^T$ we have $\nabla f_{5} = \left(\begin{array}{c}
	1.767472 \\
	-1.888581
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{5} = 0.01 \cdot \left(\begin{array}{c}
	1.767472 \\
-1.888581
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
0.017675 \\
-0.018886
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.586638e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_6 = w_5 - a\cdot \nabla f_5 =  \left[\begin{array}{c}
	1.000\\
	-0.712
\end{array}\right] - \left(\begin{array}{c}
0.017675 \\
-0.018886
\end{array}\right) \rightarrow
w_6 = \left[\begin{array}{c}
	0.982\\
	-0.694
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 6}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(6\right) = \left[0.982, -0.694\right]^T$ we have $\nabla f_{6} = \left(\begin{array}{c}
	1.744770 \\
	-1.825515
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{6} = 0.01 \cdot \left(\begin{array}{c}
	1.744770 \\
   -1.825515
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
0.017448 \\
-0.018255
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.525218e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_7 = w_6 - a\cdot \nabla f_6 =  \left[\begin{array}{c}
	0.982\\
	-0.694
\end{array}\right] - \left(\begin{array}{c}
0.017448 \\
-0.018255
\end{array}\right) \rightarrow
w_7 = \left[\begin{array}{c}
	0.965\\
	-0.675
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 7}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(7\right) = \left[0.965, -0.675\right]^T$ we have $\nabla f_{7} = \left(\begin{array}{c}
	1.721644 \\
-1.765466
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{7} = 0.01 \cdot \left(\begin{array}{c}
	1.721644 \\
-1.765466
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
0.017216 \\
-0.017655
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.465954e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_8 = w_7 - a\cdot \nabla f_7 =  \left[\begin{array}{c}
	0.965\\
-0.675
\end{array}\right] - \left(\begin{array}{c}
0.017216 \\
-0.017655
\end{array}\right) \rightarrow
w_8 = \left[\begin{array}{c}
	0.947\\
	-0.658
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 8}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(8\right) = \left[0.965, -0.675\right]^T$ we have $\nabla f_{8} = \left(\begin{array}{c}
	1.698186 \\
	-1.708205
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{8} = 0.01 \cdot \left(\begin{array}{c}
	1.698186 \\
-1.708205
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
	0.016982 \\
	-0.017082
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.408693e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_9 = w_8 - a\cdot \nabla f_8 =  \left[\begin{array}{c}
	0.947\\
	-0.658
\end{array}\right] - \left(\begin{array}{c}
	0.016982 \\
-0.017082
\end{array}\right) \rightarrow
w_9 = \left[\begin{array}{c}
	0.930\\
	-0.641
\end{array}\right]
\]
\\[4mm]

\begin{center}
	\underline{\textit{ITERATION k = 9}}
\end{center}

\underline{Step 1: Compute Gradient}\\
\(\nabla f(w_1,w_2) = \left(\begin{array}{c}
	\dfrac{\partial f}{\partial w_1} \\[4mm]
	\dfrac{\partial f}{\partial w_2}
\end{array}\right)\) $= \left(\begin{array}{c}
	2.5w_1 + w_2 + 2(0.5w_1+w_2)^3\\[1mm]
	w_1 + 4w_2 + 4(0.5w_1+w_2)^3
\end{array}\right)$ \\[3mm]

where at the point $w\left(9\right) = \left[0.930,	-0.641\right]^T$ we have $\nabla f_{9} = \left(\begin{array}{c}
	1.674479 \\
	-1.653529
\end{array}\right)$
\\[4mm]

\underline{Step 2: Step size}
\[
step = a \cdot \nabla f_{9} = 0.01 \cdot \left(\begin{array}{c}
	1.674479 \\
-1.653529
\end{array}\right) \rightarrow step =\left(\begin{array}{c}
	0.016745 \\
	-0.016535
\end{array}\right)
\]
\\[4mm]

\underline{Step 3: Check Convergence}\\
The norm of the step size is $\| step \| = \num{2.353304e-02} > tol$, so we update $w$
\\[4mm]

\underline{Step 4: Update}
\[ 
w_{10} = w_9 - a\cdot \nabla f_9 =  \left[\begin{array}{c}
	0.930\\
	-0.641
\end{array}\right] - \left(\begin{array}{c}
	0.016745 \\
-0.016535
\end{array}\right) \rightarrow
w_{10} = \left[\begin{array}{c}
	0.914\\
	-0.624
\end{array}\right]
\]
\\[4mm]

We can see that the algorithm doesn't converge yet, but it stopped due to the finite number of iterations ($Max iterations = 10$). With the help of our code we can conclude that the algorithm trully converges after \underline{506} iterations.

To sum up, after $10$ iterations, the Gradient Descent converges at $w_{10} = \left[\begin{array}{c}
	0.914\\
	-0.624
\end{array}\right]
$, close to the minimum $w = \left[\begin{array}{c}
	0\\
	0
\end{array}\right]$.\\

As we can observe, the difference between the final converged point of the 2 algorithms is substantial.
Conjugate Gradient converges in significantly fewer iterations than Gradient Descent because CG's uses a more sophisticated approach to choosing search directions and step sizes. This allows it to navigate through the geometry of the function more efficiently, leading to quicker convergence, particularly for certain classes of functions or higher-dimensional problems, such as our problem.


